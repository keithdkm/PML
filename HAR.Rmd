---
title: "A Machine Algorithm for Human Activity Recognition"
author: "Keith Miller"
date: "Monday, June 15, 2015"
output: html_document
---
## Executive Summary
This report describes and implements a machine-learning algorithm that uses data from a personal activity monitor to determine if the user of the device is performing a particular exercise in the correct way.  

A machine algorithm was created that could predict the exercise being performed with an out-of sample error rate of 1%.  When tested against 20 sample prediction problems it successfully predicted all 20.

##Qualitative Activity Recognition

Today, personal activity monitors collect information about how ***much*** of a particular exercise the wearer is doing but not how ***well*** they are doing it.  This project is an attempt to use data from movement sensors to determine if a user is performing the exercise in the correect way.

The exercise in question is barbell lifts and the data was gathered from acceleromters and gyroscopes placed on the belt, forearm, arm and dumbell of six different participants.  

The participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3dAGZdbd3
```{r Libraries, echo = FALSE, warning=FALSE, message=FALSE}
library(caret)
library(AppliedPredictiveModeling)
library(ggplot2)
library(plyr)
library(e1071)
library(doParallel)
library(knitr)
```


```{r Data Loading , echo= FALSE}
#Assumes that the training data is in the working directory in the pml-training.csv file
setwd("C:/Users/Keith/Google Drive/R/Practical Machine Learning/Project")
train<-read.csv("C:/Users/Keith/Google Drive/R/Practical Machine Learning/Project/pml-training.csv")
test<- read.csv("C:/Users/Keith/Google Drive/R/Practical Machine Learning/Project/pml-testing.csv")
dims<-dim(train)

```

##Feature Extraction

The selection of the features that could be used in the training data is described in ref 1. 

For feature extraction we used a sliding window approach
with different lengths from 0.5 second to 2.5 seconds, with
0.5 second overlap. In each step of the sliding window approach
we calculated features on the Euler angles (roll, pitch
and yaw), as well as the raw accelerometer, gyroscope and
magnetometer readings. For the Euler angles of each of the
four sensors we calculated eight features: mean, variance,
standard deviation, max, min, amplitude, kurtosis and skewness,
generating in total 96 derived feature sets.




##Exploratory Data Analysis and Pre-Processing

Two datasets were provided.  The training set is `r dims[1]` observations of `r dims[2]-1` features and 1 categorical response (Levels A-E).   The testing set has just 20 observations of the `r dims[2]-1` features and an id number for each.  This latter set was put aside for one-time final testing of the model.  After some initial modelling work, it was determined that using 50% of the data for model selection and tuning was adequate and the remainder could be set aside for cross-validation and out-of-sample error rate estimation.  All the pre-processing that was applied to the training set was also be applied validation and test sets.  

```{r Create Training and cross-Validation Sets}
# Split training datasets into train and validation sets
set.seed(601)
trainrecords<-createDataPartition(train$classe,p=0.5,list = F)
trainset<-train[trainrecords,]
validset<-train[-trainrecords,]


```

Reviewing the data, it was determined that many of the features have a large number of NA data and/or a large number of blank values. These columns will be removed from the dataset.  Further, the features in columns 1-7 will not  be included in a prediction model as they do not characterize the behavior that we are trying to assess - they relate to the time of day, the subject performing the exercise and the group of results that is summarized by the observation.
```{r Columns 1-7 sample}
trainset[1:5,1:7]

```
```{r Exploratory Data Analysis}
#identifies those variables where more than 50% of the values are NA or blank and removes them

perc.na<-function(x) sum(is.na(x))/length(x)*100
perc.blank<-function(x) sum(x=="")/length(x)*100
NAcols<-apply(trainset,2,perc.na)   
Blankcols<-apply(trainset,2,perc.blank)

par(mfrow=(c(1,2)))
plot(NAcols, xlab = "Column Number", ylab = "Percentage NA values")
plot(Blankcols,xlab = "Column Number", ylab = "Percentage Blank values")
```

All columns in the upper groups in these plots will be removed from the dataset
```{r Remove columns with missng data}
badcols<-c(which(NAcols>0.9),which(Blankcols>0.9))
trainset<- trainset[,-c(1:7,badcols)]
validset<- validset[,-c(1:7,badcols)]   #same changes made to validation set
test    <- test[,-c(1:7,badcols)]  #same changes made to test set
```

Features that have little or no variability and variables that are highly correlated with other variables can cause problems in certain models.  These features were also removed from the dataset.

```{r Remove zero variance and highly corellated variables }
#check for zero-variance predictors
zero.cols<-nearZeroVar(trainset)
if (length(zero.cols)>0) {trainset<-trainset[,-zero.cols];validset<-validset[,-zero.cols];test<-test[,-zero.cols]}

#check for multi-collinearity and remove highly correlated variables
ytrain<-trainset$classe
yvalid<-validset$classe
trainset$classe<-NULL
validset$classe<-NULL
M<-cor(trainset)
cor.cols<-findCorrelation(M,0.9)
if (length(cor.cols)>0){trainset<-trainset[,-cor.cols];validset<-validset[,-cor.cols];test<-test[,-cor.cols]}
```

 This leaves `r dim(trainset)[[2]]` features which will be the potential predictors used for the model.



##Model Selection

The caret package in R creates a consistent way of accessing all the functions and models that R has to offer, offers a set of tools that semi-automates tuning of parameters and allows parallel testing of models on multi-core machines thus speeding the process of modle selection.  


The approach used was to first apply the train function in caret in its default mode, not giving it any parameters selecting model type or cross-validation approach.   is to use cross-validation within the caret package to determine which predictors we will use and tune the performance of the model and then apply the model oe-time to the validation set to estimate out-of-sample error rates.   We will use prediction accuracy as the measure of model effectiveness when choosing the model.  


Initially the 
```{r Model Performance, cache =TRUE}
# trellis.par.set(caretTheme())
cl <- makeCluster(detectCores())
registerDoParallel(cl)
firsttry<-train(ytrain~.,data=trainset)


confusionMatrix(firsttry)

# rfallfitcv10<-trainset(classe~.,
#                      data=trainset,
#                      method = 'rf', 
#                      trControl = trainControl(method='cv',
#                                               number = 10 ))



 



# ConfusionMatrix(rfallfitcv10)

#plots the importance of the top 20 variables in the model 
# plot(varImp(rfallfitcv10),top =10)

##This code compares the differenct models
# cvValues <- resamples(list(Linear_Model = lmTune, 
#                            Random_Forest_CV10 = rffit,
#                            Gradient_Boosting = gbmTune
# summary(cvValues)                           
                           
```

```{r Generate Answers for Submission, echo=FALSE}
#writes output files containing the answer to each question
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)

setwd("C:/Users/Keith/Google Drive/R/Practical Machine Learning/Project/results")
    
    
    }}
    
```




##Approach described in lectures

define error rate (type I/type II)
split data into:
training, testing, validation (optional)
pick features from the training set
use cross-validation
pick prediction function (model) on the training set
use cross-validation
if no validation set
apply 1 time to test set
if there is a validation set
apply to test set and refine
apply 1 time to validation

## References
1. Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.


Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3dB0epBGD